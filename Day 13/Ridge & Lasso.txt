Hi Everyone

Day 13 of #66DaysOfData challenge

Today I have learnt about Ridge and Lasso Regression Indepth Intuition

-> Lasso - It helps in reducing the overfitting by introducing a parameter called 位(lamda). 
It tries to penalize the best line by moving towards 0. The 位 value is choosen using Cross Validation and this technique is also used for Feature Selection. 
The formula is cost function * 位(slope)^2.

-> Ridge - This is same as Lasso with slight difference in formula. 
Here the best line move nearer to 0. 
The formula is cost function * 位|slope|.

Here the cost function states the linear regression formula.